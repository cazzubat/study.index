{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um Chatbot com IA (Parte 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ O que √© um Chatbot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um chatbot √© um programa de computador que simula uma conversa humana, podendo responder perguntas, fornecer informa√ß√µes e at√© realizar a√ß√µes automatizadas. Ele pode funcionar por texto (ChatGPT ou DeepSeek) ou voz (como a Alexa ou o Google Assistente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Como um Chatbot Funciona?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um chatbot pode funcionar de duas formas principais:\n",
    "\n",
    "1Ô∏è‚É£ Chatbots Baseados em Regras (Simples):\n",
    "Esses bots seguem um conjunto fixo de regras. Eles s√≥ entendem perguntas espec√≠ficas e respondem com respostas pr√©-definidas.\n",
    "\n",
    "‚úÖ Exemplo:\n",
    "- üë§ Usu√°rio: \"Qual o hor√°rio de funcionamento?\"\n",
    "- ü§ñ Bot: \"Nosso hor√°rio √© das 9h √†s 18h, de segunda a sexta.\"\n",
    "\n",
    "üìå Como funciona?\n",
    "\n",
    "Ele verifica se a pergunta corresponde a um padr√£o pr√©-programado.\n",
    "Responde com uma resposta fixa.\n",
    "Desvantagem: Se a pergunta for diferente do esperado, ele pode n√£o entender.\n",
    "\n",
    "2Ô∏è‚É£ Chatbots com Intelig√™ncia Artificial (IA/NLP)\n",
    "Esses bots usam Processamento de Linguagem Natural (NLP) e Machine Learning (ML) para entender frases de forma mais natural.\n",
    "\n",
    "‚úÖ Exemplo:\n",
    "- üë§ Usu√°rio: \"Me fala sobre o hor√°rio de voc√™s?\"\n",
    "- ü§ñ Bot: \"Nosso atendimento √© das 9h √†s 18h de segunda a sexta-feira.\"\n",
    "\n",
    "üìå Como funciona?\n",
    "\n",
    "Ele transforma o texto do usu√°rio em um formato compreens√≠vel.\n",
    "Usa modelos de IA para interpretar o significado.\n",
    "Gera uma resposta din√¢mica com base no contexto.\n",
    "Aprende com intera√ß√µes anteriores para melhorar suas respostas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Tecnologias Usadas em Chatbots\n",
    "Chatbots podem ser constru√≠dos com v√°rias tecnologias, como:\n",
    "\n",
    "- üîπ NLP (Processamento de Linguagem Natural) ‚Üí Para entender a linguagem humana (Ex: spaCy, NLTK, Hugging Face).\n",
    "- üîπ Redes Neurais ‚Üí Para respostas mais inteligentes e aprendizado cont√≠nuo (Ex: ChatGPT).\n",
    "- üîπ APIs de IA ‚Üí Como Dialogflow (Google), Watson Assistant (IBM), Microsoft Bot Framework.\n",
    "- üîπ Frameworks para Chatbots ‚Üí Rasa, ChatterBot, BotPress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![langchain](langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå O que √© LangChain e como ele melhora os Chatbots?\n",
    "\n",
    "Vamos aprender a usar o LangChain para criar um sistema de Recupera√ß√£o Aumentada com Gera√ß√£o (RAG), onde um chatbot inteligente responde clientes com base em textos previamente armazenados e uma LLM gratuita.\n",
    "\n",
    "Defini√ß√£o: LangChain √© um framework para construir aplica√ß√µes com Large Language Models (LLMs), como ChatGPT, LLaMA e Mistral.\n",
    "\n",
    "üìå O que √© RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "Aplica√ß√£o real: Empresas usam RAG para criar agentes de atendimento ao cliente, que interpretam documentos, bases de conhecimento e FAQs.\n",
    "\n",
    "O ChatGPT pode responder com base em conhecimento geral.\n",
    "Um RAG pode responder com base em documentos internos da empresa (exemplo: pol√≠ticas de reembolso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Vis√£o geral das LLMs gratuitas para APIs e locais\n",
    "\n",
    "APIs de LLMs gratuitas:\n",
    "\n",
    "- OpenAI GPT-3.5/GPT-4 (com limites gratuitos).\n",
    "- Mistral e Hugging Face API (modelos open-source).\n",
    "- Together.ai (fornece acesso gratuito a v√°rias LLMs).\n",
    "- Google AI Studio: Disponibiliza modelos de linguagem que podem ser utilizados gratuitamente para diversas aplica√ß√µes. \n",
    "- Hugging Face: Oferece uma ampla gama de modelos de linguagem que podem ser acessados gratuitamente atrav√©s de sua API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG](RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![explan](explan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Chamando uma API de LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain langchain-community openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Configurando a API da OpenAI (use sua chave de API)\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=api_key)\n",
    "\n",
    "# Enviando uma pergunta para a LLM\n",
    "resposta = chat([HumanMessage(content=\"Como funciona um chatbot de atendimento ao cliente?\")])\n",
    "\n",
    "print(\"Resposta da IA:\", resposta.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-google-genai google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m resposta = chat.invoke(\u001b[33m\"\u001b[39m\u001b[33mComo funciona um chatbot de atendimento ao cliente?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Exibir a resposta\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mResposta da IA:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mresposta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "# Configurar a API Key da Gemini (Google AI)\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")  \n",
    "\n",
    "# Inicializando o modelo Gemini 1.5 Flash\n",
    "chat = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=api_key)\n",
    "\n",
    "# Enviando uma pergunta para a LLM\n",
    "resposta = chat.invoke(\"Como funciona um chatbot de atendimento ao cliente?\")\n",
    "\n",
    "# Exibir a resposta\n",
    "print(\"Resposta da IA:\", resposta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: Explique o que √© uma IA generativa.\n",
      "\n",
      "A Intelig√™ncia Artificial Generativa (AI Generativa) √© uma subdisciplina da Intelig√™ncia Artificial que utiliza modelos de aprendizagem de m√°quina capazes de gerar informa√ß√µes novas e originais, sem serem direcionados por um humano. Essas informa√ß√µes podem ser texto, imagens, √°udio ou outros formatos de dados.\n",
      "\n",
      "AI Generativas s√£o capazes de analisar e processar grande quantidades de dados existentes e criar novas informa√ß√µes baseadas nesses dados. Eles aprendem a gerar novas informa√ß√µes atrav√©s de algoritmos de aprendizagem profunda, como redes neurais convolucionais (CNNs) e redes recurrentes (RNNs), que podem descobrir padr√µes e estruturas nas informa√ß√µes existentes e usar essas descobertas para gerar novas informa√ß√µes semelhantes.\n",
      "\n",
      "Uma aplica√ß√£o comum de AI Generativas √© a cria√ß√£o de conte√∫do multim√©dia, como imagens, v√≠deos ou textos. Por exemplo, um modelo de AI Generativa pode analisar milhares de imagens de c√£es e ent√£o gerar uma nova imagem de um c√£o que nunca foi visto antes, baseado nos padr√µes e caracter√≠sticas aprendidos dos exemplos existentes. Outra aplica√ß√£o √© a cria√ß√£o de texto, onde o modelo pode analisar milhares de artigos e ent√£o gerar um novo artigo sobre um determinado assunto.\n",
      "\n",
      "Em resumo, uma AI Generativa √© um sistema inteligente capaz de gerar informa√ß√µes novas e originais, sem ser direcionado por um humano, atrav√©s do aprendizado de modelos de m√°quina baseados em grandes quantidades de dados existentes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "\n",
    "# Configurar o modelo da Hugging Face\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",  # Escolha o modelo\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},  # Configura√ß√µes\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")  # Usa a API Key\n",
    ")\n",
    "\n",
    "# Testando o modelo\n",
    "resposta = llm.invoke(\"Explique o que √© uma IA generativa.\")\n",
    "print(\"Resposta:\", resposta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Rodando Modelos LLM Locais com LLaMA e LangChain\n",
    "\n",
    "- Para rodar LLMs no pr√≥prio computador sem depender de APIs.\n",
    "\n",
    "- Instalando o Ollama (para rodar LLaMA localmente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo a passo para isntalar o ollama localmente:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Acesse o site https://ollama.com\n",
    "- Baixe o instalador de LLMs\n",
    "- Instale na m√°quina\n",
    "- Escolha a LLM que ir√° usar\n",
    "- Instale a LLM no bash do Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ollama' nÔøΩo ÔøΩ reconhecido como um comando interno\n",
      "ou externo, um programa operÔøΩvel ou um arquivo em lotes.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîπ Mistral 7B / Mixtral\n",
    "\n",
    "- Pontos fortes: √ìtimo para conversa√ß√£o e respostas estruturadas.\n",
    "- Onde encontrar: Dispon√≠vel no Ollama (ollama pull mistral).\n",
    "- Ideal para: Chatbots empresariais e suporte t√©cnico.\n",
    "\n",
    "Como usar no LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O Brasil foi descoberto pelos portugueses. Por volta do ano de 1500, a expedi√ß√£o liderada por Pedro √Ålvares Cabral, como parte da frota enviada pelo Rei D. Manuel I de Portugal, desembarcou no atual estado brasileiro de Esp√≠rito Santo, marcando o in√≠cio do processo de coloniza√ß√£o portuguesa do territ√≥rio brasileiro.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Inicializando a LLM local\n",
    "llm = Ollama(model=\"mistral\")\n",
    "resposta = llm(\"Quem descobriu o Brasil?\")\n",
    "print(resposta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîπ LLaMA 3 (Meta)\n",
    "\n",
    "- Pontos fortes: √ìtima compreens√£o de contexto e menor consumo de mem√≥ria.\n",
    "- Onde encontrar: Dispon√≠vel via Ollama (ollama pull llama3).\n",
    "\n",
    "Como usar no LangChain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Inicializando a LLM local\n",
    "llm = Ollama(model=\"llama3\")\n",
    "resposta = llm(\"O que √© NLP?\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Como Configurar um Chatbot Estruturado com RAG no LangChain\n",
    "\n",
    "Agora, vamos configurar um pipeline de RAG para seu chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Criando a Base de Conhecimento\n",
    "\n",
    "Agora, vamos configurar um pipeline de RAG para seu chatbot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banco de vetores criado com sucesso usando Mistral!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Criando documentos para o FAISS\n",
    "documentos = [\n",
    "    Document(page_content=\"Nosso suporte est√° dispon√≠vel de segunda a sexta, das 9h √†s 18h.\"),\n",
    "    Document(page_content=\"O reembolso pode ser solicitado em at√© 7 dias ap√≥s a compra.\"),\n",
    "    Document(page_content=\"Para falar com um atendente, envie um e-mail para suporte@empresa.com.\"),\n",
    "]\n",
    "\n",
    "# Usando embeddings locais com Ollama (modelo Mistral)\n",
    "vectorstore = FAISS.from_documents(documentos, OllamaEmbeddings(model=\"mistral\"))\n",
    "\n",
    "print(\"Banco de vetores criado com sucesso usando Mistral!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Isso cria um banco de vetores com informa√ß√µes do suporte ao cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Criando o Chatbot RAG\n",
    "\n",
    "Agora, criamos um agente que busca respostas antes de responder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\AppData\\Local\\Temp\\ipykernel_18900\\405955736.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  resposta = qa.run(pergunta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta do Chatbot:  O hor√°rio de atendimento do nosso suporte √© de segunda a sexta, das 9h √†s 18h.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama  # Import correto para Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Criando a cadeia de perguntas e respostas com recupera√ß√£o de contexto\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Fazendo uma pergunta ao chatbot\n",
    "pergunta = \"Qual o hor√°rio de atendimento?\"\n",
    "resposta = qa.run(pergunta)\n",
    "\n",
    "print(\"Resposta do Chatbot:\", resposta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclus√£o\n",
    "\n",
    "Qual LLM Escolher?\n",
    "\n",
    "![taela_llm](taela_llm.png)\n",
    "\n",
    "- ‚úÖ Se precisar de um chatbot local, v√° de Mistral 7B ou LLaMA 3.\n",
    "- ‚úÖ Se precisar de alta precis√£o, use GPT-4 Turbo ou Cohere API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
