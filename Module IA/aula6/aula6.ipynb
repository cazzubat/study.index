{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Como Configurar um Chatbot Estruturado com RAG no LangChain\n",
    "\n",
    "Agora, vamos configurar um pipeline de RAG para seu chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Criando a Base de Conhecimento (LOCAL)\n",
    "\n",
    "Agora, vamos configurar um pipeline de RAG para seu chatbot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\AppData\\Local\\Temp\\ipykernel_26464\\1179460957.py:13: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  vectorstore = FAISS.from_documents(documentos, OllamaEmbeddings(model=\"mistral\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banco de vetores criado com sucesso usando Mistral!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Criando documentos para o FAISS\n",
    "documentos = [\n",
    "    Document(page_content=\"Nosso suporte est√° dispon√≠vel de segunda a sexta, das 9h √†s 18h.\"),\n",
    "    Document(page_content=\"O reembolso pode ser solicitado em at√© 7 dias ap√≥s a compra.\"),\n",
    "    Document(page_content=\"Para falar com um atendente, envie um e-mail para suporte@empresa.com.\"),\n",
    "]\n",
    "\n",
    "# Usando embeddings locais com Ollama (modelo Mistral)\n",
    "vectorstore = FAISS.from_documents(documentos, OllamaEmbeddings(model=\"mistral\"))\n",
    "\n",
    "print(\"Banco de vetores criado com sucesso usando Mistral!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Isso cria um banco de vetores com informa√ß√µes do suporte ao cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Criando o Chatbot RAG (LOCAL)\n",
    "\n",
    "Agora, criamos um agente que busca respostas antes de responder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\AppData\\Local\\Temp\\ipykernel_26464\\405955736.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")\n",
      "C:\\Users\\Eric\\AppData\\Local\\Temp\\ipykernel_26464\\405955736.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  resposta = qa.run(pergunta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta do Chatbot:  O hor√°rio de atendimento √© de segunda a sexta, das 9h √†s 18h.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama  # Import correto para Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Criando a cadeia de perguntas e respostas com recupera√ß√£o de contexto\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Fazendo uma pergunta ao chatbot\n",
    "pergunta = \"Qual o hor√°rio de atendimento?\"\n",
    "resposta = qa.run(pergunta)\n",
    "\n",
    "print(\"Resposta do Chatbot:\", resposta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2.1: Criando o Chatbot RAG (VIA API)\n",
    "\n",
    "Agora, criamos um agente que busca respostas antes de responder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclus√£o\n",
    "\n",
    "Qual LLM Escolher?\n",
    "\n",
    "![taela_llm](taela_llm.png)\n",
    "\n",
    "- ‚úÖ Se precisar de um chatbot local, v√° de Mistral 7B ou LLaMA 3.\n",
    "- ‚úÖ Se precisar de alta precis√£o, use GPT-4 Turbo ou Cohere API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2.1: Criando o Chatbot RAG (VIA API)\n",
    "\n",
    "Agora, criamos um agente que busca respostas antes de responder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\AppData\\Local\\Temp\\ipykernel_26464\\2655203033.py:10: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  llm = HuggingFaceHub(\n",
      "C:\\Users\\Eric\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Eric\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Resposta: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Nosso suporte est√° dispon√≠vel de segunda a sexta, das 9h √†s 18h.\n",
      "\n",
      "O reembolso pode ser solicitado em at√© 7 dias ap√≥s a compra.\n",
      "\n",
      "Para falar com um atendente, envie um e-mail para suporte@empresa.com.\n",
      "\n",
      "Question: Como posso entrar em contato com o suporte?\n",
      "Helpful Answer: Voc√™ pode entrar em contato com o suporte enviando um e-mail para suporte@empresa.com. O suporte est√° dispon√≠vel de segunda a sexta, das 9h √†s 18h. Se voc√™ quiser solicitar um reembolso, fa√ßa isso dentro dos 7 primeiros dias ap√≥s a compra.\n",
      "üìÑ Fonte(s): ['Nosso suporte est√° dispon√≠vel de segunda a sexta, das 9h √†s 18h.', 'O reembolso pode ser solicitado em at√© 7 dias ap√≥s a compra.', 'Para falar com um atendente, envie um e-mail para suporte@empresa.com.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings  # ou SentenceTransformersEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "# 2. Instanciar o modelo LLM via Hugging Face\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# 3. Criar os documentos de conhecimento\n",
    "documentos = [\n",
    "    Document(page_content=\"Nosso suporte est√° dispon√≠vel de segunda a sexta, das 9h √†s 18h.\"),\n",
    "    Document(page_content=\"O reembolso pode ser solicitado em at√© 7 dias ap√≥s a compra.\"),\n",
    "    Document(page_content=\"Para falar com um atendente, envie um e-mail para suporte@empresa.com.\"),\n",
    "]\n",
    "\n",
    "# 4. Gerar embeddings e criar o FAISS\n",
    "embedding_function = OllamaEmbeddings(model=\"mistral\")  # ou outro modelo local\n",
    "vectorstore = FAISS.from_documents(documentos, embedding=embedding_function)\n",
    "\n",
    "# 5. Criar o pipeline RAG com RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 6. Fazer uma pergunta\n",
    "pergunta = \"Como posso entrar em contato com o suporte?\"\n",
    "resposta = qa_chain.invoke(pergunta)\n",
    "\n",
    "print(\"üîé Resposta:\", resposta[\"result\"])\n",
    "print(\"üìÑ Fonte(s):\", [doc.page_content for doc in resposta[\"source_documents\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì O embedding precisa ser feito com modelo local?\n",
    "N√£o. Voc√™ pode usar modelos de embeddings remotos tamb√©m! üöÄ\n",
    "\n",
    "LangChain permite que voc√™ escolha entre modelos locais ou via API para gerar os vetores de embedding. Tudo depende de custo, privacidade, velocidade e conectividade.\n",
    "\n",
    "Se quiser usar embeddings da Hugging Face tamb√©m (em vez do OllamaEmbeddings), troque para:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Resumo: posso usar..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tabela.png](tabela.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üöÄ Agora voc√™ tem:\n",
    "\n",
    "Um modelo remoto (HuggingFaceHub) ou local (Mistral).\n",
    "\n",
    "Embeddings locais ou remotos.\n",
    "\n",
    "Um pipeline completo de RAG.\n",
    "\n",
    "Pronto para integrar com imagem, consulta com cliente final e muito mais!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3: Personaliza√ß√£o de Chatbots\n",
    "\n",
    "Por que personalizar um chatbot?\n",
    "\n",
    "- Humaniza√ß√£o no atendimento.\n",
    "- Manter a identidade e o tom da marca.\n",
    "- Transmitir confian√ßa e profissionalismo.\n",
    "\n",
    "üìå Exemplo:\n",
    "\n",
    "Compare as duas frases de uma I.A.:\n",
    "\n",
    "‚ùå Gen√©rico: ‚ÄúN√£o sei, tente outra pergunta.‚Äù\n",
    "\n",
    "‚úÖ Profissional: ‚ÄúAinda n√£o tenho essa informa√ß√£o, mas posso te encaminhar ao setor respons√°vel. Deseja prosseguir?‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Criando um Perfil de Funcion√°rio para o Chatbot \n",
    "\n",
    "Estrat√©gia: Vamos criar um prompt-base com instru√ß√µes claras para a LLM responder como um funcion√°rio da empresa.\n",
    "\n",
    "Exemplo de system_prompt personalizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Voc√™ √© um atendente virtual da empresa TechSolutions. \n",
    "Seu papel √© responder de forma clara, educada e objetiva, \n",
    "como se fosse um funcion√°rio treinado no setor de suporte ao cliente.\n",
    "\n",
    "- Use uma linguagem formal e acolhedora.\n",
    "- Se a resposta n√£o for encontrada na base de conhecimento, oriente o usu√°rio a entrar em contato pelo e-mail suporte@techsolutions.com.\n",
    "- Nunca invente informa√ß√µes. Seja honesto e direto.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ VERS√ÉO 1 Usando LLM local com Ollama (modelo mistral)\n",
    "\n",
    "```python\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 1. Usando modelo local com Ollama (instale e rode com: `ollama run mistral`)\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# 2. Definindo o prompt personalizado\n",
    "system_prompt = \"Voc√™ √© um assistente inteligente que sempre responde em portugu√™s.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ VERS√ÉO 2 ‚Äì Usando LLM da Hugging Face (mistralai/Mistral-7B-Instruct-v0.2)\n",
    "\n",
    "```python\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# 1. Instanciar o LLM via Hugging Face Hub\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 512},\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# 2. Prompt personalizado\n",
    "system_prompt = \"Voc√™ √© um assistente inteligente que sempre responde em portugu√™s.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Personalizando as Respostas com a Base de Conhecimento\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ VERS√ÉO 1 ‚Äì Usando embeddings locais com Ollama (modelo mistral)\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Documentos de entrada\n",
    "documentos = [\n",
    "    Document(page_content=\"Prezados clientes, nosso prazo de reembolso √© de at√© 7 dias √∫teis ap√≥s a solicita√ß√£o.\"),\n",
    "    Document(page_content=\"O atendimento funciona de segunda a sexta-feira, das 9h √†s 18h.\"),\n",
    "    Document(page_content=\"Para segunda via de boleto, entre em contato pelo nosso portal ou e-mail.\")\n",
    "]\n",
    "\n",
    "# Embeddings com modelo local via Ollama (precisa rodar: `ollama run mistral`)\n",
    "embedding_function = OllamaEmbeddings(model=\"mistral\")\n",
    "\n",
    "# Cria√ß√£o do vetor FAISS\n",
    "vectorstore = FAISS.from_documents(documentos, embedding=embedding_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ VERS√ÉO 2 ‚Äì Usando embeddings da Hugging Face via API (ex: all-MiniLM-L6-v2)\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Configure sua chave da Hugging Face\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"sua_chave_aqui\"\n",
    "\n",
    "# Documentos de entrada\n",
    "documentos = [\n",
    "    Document(page_content=\"Prezados clientes, nosso prazo de reembolso √© de at√© 7 dias √∫teis ap√≥s a solicita√ß√£o.\"),\n",
    "    Document(page_content=\"O atendimento funciona de segunda a sexta-feira, das 9h √†s 18h.\"),\n",
    "    Document(page_content=\"Para segunda via de boleto, entre em contato pelo nosso portal ou e-mail.\")\n",
    "]\n",
    "\n",
    "# Embeddings via Hugging Face API (modelo leve e gratuito)\n",
    "embedding_function = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Cria√ß√£o do vetor FAISS\n",
    "vectorstore = FAISS.from_documents(documentos, embedding=embedding_function)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Finaliza√ß√£o com mont√°gem da cadeia com contexto + estilo profissional\n",
    "\n",
    "‚úÖ O chatbot agora responder√° com base na base de conhecimento e no tom de voz definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chatbot = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√≥digo finalizado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ VERS√ÉO 1 ‚Äì Usando LLM local com Ollama + embeddings com Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1. LLM local via Ollama\n",
    "llm = Ollama(model=\"mistral\", temperature=0)\n",
    "\n",
    "# 2. Embeddings com modelo local via Ollama\n",
    "embedding_function = OllamaEmbeddings(model=\"mistral\")\n",
    "\n",
    "# 3. Base de conhecimento\n",
    "documentos = [\n",
    "    Document(page_content=\"Prezados clientes, nosso prazo de reembolso √© de at√© 7 dias √∫teis ap√≥s a solicita√ß√£o.\"),\n",
    "    Document(page_content=\"O atendimento funciona de segunda a sexta-feira, das 9h √†s 18h.\"),\n",
    "    Document(page_content=\"Para segunda via de boleto, entre em contato pelo nosso portal ou e-mail.\")\n",
    "]\n",
    "vectorstore = FAISS.from_documents(documentos, embedding_function)\n",
    "\n",
    "# 4. Mem√≥ria da conversa\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 5. Prompt\n",
    "system_prompt = \"Voc√™ √© um atendente educado e eficiente que responde sempre em portugu√™s.\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 6. Cadeia de conversa com RAG\n",
    "chatbot = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# 7. Consulta\n",
    "pergunta = \"Como posso pedir a segunda via do boleto?\"\n",
    "resposta = chatbot.invoke({\"question\": pergunta})\n",
    "print(\"üß† Resposta:\", resposta[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ VERS√ÉO 2 ‚Äì Usando LLM da Hugging Face (via API) + embeddings via HF API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1. Configurar chave da Hugging Face\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"sua_chave_aqui\"\n",
    "\n",
    "# 2. LLM da Hugging Face (API)\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 512},\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    ")\n",
    "\n",
    "# 3. Embeddings via HF Inference API\n",
    "embedding_function = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# 4. Base de documentos\n",
    "documentos = [\n",
    "    Document(page_content=\"Prezados clientes, nosso prazo de reembolso √© de at√© 7 dias √∫teis ap√≥s a solicita√ß√£o.\"),\n",
    "    Document(page_content=\"O atendimento funciona de segunda a sexta-feira, das 9h √†s 18h.\"),\n",
    "    Document(page_content=\"Para segunda via de boleto, entre em contato pelo nosso portal ou e-mail.\")\n",
    "]\n",
    "vectorstore = FAISS.from_documents(documentos, embedding_function)\n",
    "\n",
    "# 5. Mem√≥ria de conversa√ß√£o\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 6. Prompt personalizado\n",
    "system_prompt = \"Voc√™ √© um atendente educado e eficiente que responde sempre em portugu√™s.\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 7. Cadeia conversacional RAG\n",
    "chatbot = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# 8. Pergunta do usu√°rio\n",
    "pergunta = \"Como posso pedir a segunda via do boleto?\"\n",
    "resposta = chatbot.invoke({\"question\": pergunta})\n",
    "print(\"üß† Resposta:\", resposta[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
